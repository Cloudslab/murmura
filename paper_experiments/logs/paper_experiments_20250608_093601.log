2025-06-08 09:36:01,175 - INFO - Filtering experiments to dataset: ham10000
2025-06-08 09:36:01,175 - INFO - Generated 255 valid experimental configurations
2025-06-08 09:36:01,175 - INFO - üöÄ Starting 255 comprehensive experiments
2025-06-08 09:36:01,175 - INFO -    Max parallel: 1
2025-06-08 09:36:01,175 - INFO -    Output directory: paper_experiments
2025-06-08 09:36:01,175 - INFO -    Estimated total time: 22.7 hours
2025-06-08 09:36:01,175 - INFO - [1/255] Starting experiment...
2025-06-08 09:36:01,175 - INFO - üéØ Starting experiment 1: exp_0001_ham10000_federated_star_5n_no_dp
2025-06-08 09:36:01,175 - INFO -    Config: sensitive_groups attack, 5 nodes, no_dp
2025-06-08 09:38:04,693 - ERROR -    ‚ùå Training failed with traceback: Traceback (most recent call last):   File "/home/ubuntu/murmura/murmura/examples/dp_ham10000_example.py", line 490, in main     results = learning_process.execute()               ^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/ubuntu/murmura/murmura/orchestration/learning_process/federated_learning_process.py", line 234, in execute     params = ray.get(actor.get_model_parameters.remote())              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/ubuntu/miniconda3/envs/murmura/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper     return fn(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^   File "/home/ubuntu/miniconda3/envs/murmura/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File "/home/ubuntu/miniconda3/envs/murmura/lib/python3.11/site-packages/ray/_private/worker.py", line 2822, in get     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "/home/ubuntu/miniconda3/envs/murmura/lib/python3.11/site-packages/ray/_private/worker.py", line 932, in get_objects     raise value ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory. Memory on the node (IP: 10.0.4.121, ID: 50eccf61b7c78cd958bd342bc6c88b27fba5f8dcda611d1932477252) where the task (actor ID: 677878be873cad1f070d480447010000, name=VirtualClientActor.__init__, pid=213562, memory used=0.97GB) was running was 29.80GB / 30.99GB (0.961297), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 71039c53aa6ce8a5ea99b3cf3efd4c0cc76d3c13d200f44668f767f4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 10.0.4.121`. To see the logs of the worker, use `ray logs worker-71039c53aa6ce8a5ea99b3cf3efd4c0cc76d3c13d200f44668f767f4*out -ip 10.0.4.121. Top 10 memory users:
2025-06-08 09:38:04,696 - INFO - [1/255] ‚ùå ham10000/federated/star/5n | no-DP | sensitive-groups | EXPERIMENT FAILED | Error: Training failed with traceback: Traceback (most recent call last):   File "/home/ubuntu/murmura/murm | Runtime: 123.5s
2025-06-08 09:38:04,697 - INFO - [2/255] Starting experiment...
2025-06-08 09:38:04,697 - INFO - üéØ Starting experiment 2: exp_0002_ham10000_federated_complete_5n_no_dp
2025-06-08 09:38:04,697 - INFO -    Config: sensitive_groups attack, 5 nodes, no_dp
