#!/usr/bin/env python3
"""
Comprehensive Analysis and Visualization for Trust Monitor Experiments.

This module provides detailed analysis and visualization capabilities for 
experimental results, generating paper-ready figures and tables.
"""

import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, List, Optional
import argparse


class ExperimentAnalyzer:
    """Comprehensive analyzer for trust monitor experiment results."""
    
    def __init__(self, results_dir: str):
        self.results_dir = results_dir
        self.results_df = None
        self.paper_stats = None
        
        # Setup plotting style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
    def load_results(self):
        """Load experiment results and statistics."""
        
        # Load main results
        results_file = os.path.join(self.results_dir, "experiment_results.csv")
        if os.path.exists(results_file):
            self.results_df = pd.read_csv(results_file)
            # Parse config column if it's stored as string
            if 'config' in self.results_df.columns and isinstance(self.results_df['config'].iloc[0], str):
                self.results_df['config'] = self.results_df['config'].apply(json.loads)
        else:
            raise FileNotFoundError(f"Results file not found: {results_file}")
        
        # Load paper statistics
        stats_file = os.path.join(self.results_dir, "paper_statistics.json")
        if os.path.exists(stats_file):
            with open(stats_file, 'r') as f:
                self.paper_stats = json.load(f)
        
        print(f"Loaded {len(self.results_df)} experiment results")
        print(f"Successful experiments: {len(self.results_df[self.results_df['success']])}")
    
    def generate_paper_tables(self, output_dir: str = None):
        """Generate LaTeX tables for paper."""
        
        if output_dir is None:
            output_dir = os.path.join(self.results_dir, "paper_tables")
        os.makedirs(output_dir, exist_ok=True)
        
        successful_df = self.results_df[self.results_df['success'] == True]
        
        # Table 1: Detection Performance by Attack Type
        self._generate_attack_performance_table(successful_df, output_dir)\n        \n        # Table 2: Topology Comparison\n        self._generate_topology_comparison_table(successful_df, output_dir)\n        \n        # Table 3: Overhead Analysis\n        self._generate_overhead_table(successful_df, output_dir)\n        \n        # Table 4: Scalability Analysis\n        self._generate_scalability_table(successful_df, output_dir)\n        \n        print(f\"Paper tables generated in: {output_dir}\")\n    \n    def _generate_attack_performance_table(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Generate attack performance comparison table.\"\"\"\n        \n        # Group by attack type\n        attack_stats = []\n        for attack_type in ['none', 'label_flipping', 'model_poisoning', 'byzantine_gradient']:\n            attack_df = df[df['config'].apply(lambda x: x['attack_type']) == attack_type]\n            \n            if len(attack_df) > 0:\n                # Filter out infinite latencies for attack types\n                if attack_type != 'none':\n                    latency_df = attack_df[attack_df['avg_detection_latency'] != float('inf')]\n                    avg_latency = latency_df['avg_detection_latency'].mean() if len(latency_df) > 0 else None\n                    std_latency = latency_df['avg_detection_latency'].std() if len(latency_df) > 0 else None\n                else:\n                    avg_latency = None\n                    std_latency = None\n                \n                attack_stats.append({\n                    'Attack Type': attack_type.replace('_', ' ').title(),\n                    'Experiments': len(attack_df),\n                    'Precision': f\"{attack_df['precision'].mean():.3f} ± {attack_df['precision'].std():.3f}\",\n                    'Recall': f\"{attack_df['recall'].mean():.3f} ± {attack_df['recall'].std():.3f}\",\n                    'F1-Score': f\"{attack_df['f1_score'].mean():.3f} ± {attack_df['f1_score'].std():.3f}\",\n                    'Detection Latency': f\"{avg_latency:.1f} ± {std_latency:.1f}\" if avg_latency else \"N/A\",\n                    'FP Rate': f\"{attack_df['false_positive_rate'].mean():.3f} ± {attack_df['false_positive_rate'].std():.3f}\"\n                })\n        \n        attack_table = pd.DataFrame(attack_stats)\n        \n        # Save as LaTeX\n        latex_table = attack_table.to_latex(index=False, escape=False, \n                                          caption=\"Detection Performance by Attack Type\",\n                                          label=\"tab:attack_performance\")\n        \n        with open(os.path.join(output_dir, \"attack_performance_table.tex\"), 'w') as f:\n            f.write(latex_table)\n        \n        # Save as CSV for reference\n        attack_table.to_csv(os.path.join(output_dir, \"attack_performance_table.csv\"), index=False)\n    \n    def _generate_topology_comparison_table(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Generate topology comparison table.\"\"\"\n        \n        topology_stats = []\n        for topology in ['ring', 'complete', 'line']:\n            topo_df = df[df['config'].apply(lambda x: x['topology']) == topology]\n            \n            if len(topo_df) > 0:\n                # Only consider experiments with attacks for detection metrics\n                attack_df = topo_df[topo_df['config'].apply(lambda x: x['attack_type']) != 'none']\n                \n                # Latency for detected attacks only\n                if len(attack_df) > 0:\n                    latency_df = attack_df[attack_df['avg_detection_latency'] != float('inf')]\n                    avg_latency = latency_df['avg_detection_latency'].mean() if len(latency_df) > 0 else None\n                else:\n                    avg_latency = None\n                \n                topology_stats.append({\n                    'Topology': topology.title(),\n                    'Experiments': len(topo_df),\n                    'Avg Memory (MB)': f\"{topo_df['avg_memory_mb'].mean():.1f} ± {topo_df['avg_memory_mb'].std():.1f}\",\n                    'Trust Overhead (MB)': f\"{topo_df['trust_overhead_mb'].mean():.1f} ± {topo_df['trust_overhead_mb'].std():.1f}\",\n                    'CPU Usage (%)': f\"{topo_df['avg_cpu_percent'].mean():.1f} ± {topo_df['avg_cpu_percent'].std():.1f}\",\n                    'Detection Latency': f\"{avg_latency:.1f}\" if avg_latency else \"N/A\",\n                    'Trust Separation': f\"{topo_df['trust_score_separation'].mean():.2f} ± {topo_df['trust_score_separation'].std():.2f}\"\n                })\n        \n        topology_table = pd.DataFrame(topology_stats)\n        \n        # Save as LaTeX\n        latex_table = topology_table.to_latex(index=False, escape=False,\n                                            caption=\"Performance Comparison by Network Topology\",\n                                            label=\"tab:topology_comparison\")\n        \n        with open(os.path.join(output_dir, \"topology_comparison_table.tex\"), 'w') as f:\n            f.write(latex_table)\n        \n        topology_table.to_csv(os.path.join(output_dir, \"topology_comparison_table.csv\"), index=False)\n    \n    def _generate_overhead_table(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Generate overhead analysis table.\"\"\"\n        \n        overhead_stats = []\n        \n        # By dataset\n        for dataset in ['mnist', 'cifar10']:\n            dataset_df = df[df['config'].apply(lambda x: x['dataset']) == dataset]\n            \n            if len(dataset_df) > 0:\n                overhead_stats.append({\n                    'Configuration': f\"{dataset.upper()}\",\n                    'Avg Memory (MB)': f\"{dataset_df['avg_memory_mb'].mean():.1f}\",\n                    'Peak Memory (MB)': f\"{dataset_df['peak_memory_mb'].mean():.1f}\",\n                    'Trust Overhead (MB)': f\"{dataset_df['trust_overhead_mb'].mean():.1f}\",\n                    'CPU Usage (%)': f\"{dataset_df['avg_cpu_percent'].mean():.1f}\",\n                    'Communication Overhead (%)': f\"{dataset_df['communication_overhead_percent'].mean():.1f}\"\n                })\n        \n        # By number of actors\n        for num_actors in sorted(df['config'].apply(lambda x: x['num_actors']).unique()):\n            actors_df = df[df['config'].apply(lambda x: x['num_actors']) == num_actors]\n            \n            if len(actors_df) > 0:\n                overhead_stats.append({\n                    'Configuration': f\"{num_actors} Actors\",\n                    'Avg Memory (MB)': f\"{actors_df['avg_memory_mb'].mean():.1f}\",\n                    'Peak Memory (MB)': f\"{actors_df['peak_memory_mb'].mean():.1f}\",\n                    'Trust Overhead (MB)': f\"{actors_df['trust_overhead_mb'].mean():.1f}\",\n                    'CPU Usage (%)': f\"{actors_df['avg_cpu_percent'].mean():.1f}\",\n                    'Communication Overhead (%)': f\"{actors_df['communication_overhead_percent'].mean():.1f}\"\n                })\n        \n        overhead_table = pd.DataFrame(overhead_stats)\n        \n        # Save as LaTeX\n        latex_table = overhead_table.to_latex(index=False, escape=False,\n                                            caption=\"Trust Monitor Overhead Analysis\",\n                                            label=\"tab:overhead_analysis\")\n        \n        with open(os.path.join(output_dir, \"overhead_analysis_table.tex\"), 'w') as f:\n            f.write(latex_table)\n        \n        overhead_table.to_csv(os.path.join(output_dir, \"overhead_analysis_table.csv\"), index=False)\n    \n    def _generate_scalability_table(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Generate scalability analysis table.\"\"\"\n        \n        scalability_stats = []\n        num_actors_list = sorted(df['config'].apply(lambda x: x['num_actors']).unique())\n        \n        for num_actors in num_actors_list:\n            actors_df = df[df['config'].apply(lambda x: x['num_actors']) == num_actors]\n            \n            # Only consider successful experiments with attacks\n            attack_df = actors_df[\n                (actors_df['success'] == True) & \n                (actors_df['config'].apply(lambda x: x['attack_type']) != 'none')\n            ]\n            \n            if len(attack_df) > 0:\n                # Detection latency for successful detections\n                latency_df = attack_df[attack_df['avg_detection_latency'] != float('inf')]\n                avg_latency = latency_df['avg_detection_latency'].mean() if len(latency_df) > 0 else None\n                \n                scalability_stats.append({\n                    'Network Size': num_actors,\n                    'Experiments': len(actors_df),\n                    'Success Rate': f\"{len(actors_df[actors_df['success']]) / len(actors_df):.1%}\",\n                    'Avg Precision': f\"{attack_df['precision'].mean():.3f}\",\n                    'Avg Recall': f\"{attack_df['recall'].mean():.3f}\",\n                    'Detection Latency': f\"{avg_latency:.1f}\" if avg_latency else \"N/A\",\n                    'Memory per Node (MB)': f\"{attack_df['avg_memory_mb'].mean() / num_actors:.1f}\",\n                    'Trust Overhead (MB)': f\"{attack_df['trust_overhead_mb'].mean():.1f}\"\n                })\n        \n        scalability_table = pd.DataFrame(scalability_stats)\n        \n        # Save as LaTeX\n        latex_table = scalability_table.to_latex(index=False, escape=False,\n                                                caption=\"Scalability Analysis by Network Size\",\n                                                label=\"tab:scalability_analysis\")\n        \n        with open(os.path.join(output_dir, \"scalability_analysis_table.tex\"), 'w') as f:\n            f.write(latex_table)\n        \n        scalability_table.to_csv(os.path.join(output_dir, \"scalability_analysis_table.csv\"), index=False)\n    \n    def generate_paper_figures(self, output_dir: str = None):\n        \"\"\"Generate publication-ready figures.\"\"\"\n        \n        if output_dir is None:\n            output_dir = os.path.join(self.results_dir, \"paper_figures\")\n        os.makedirs(output_dir, exist_ok=True)\n        \n        successful_df = self.results_df[self.results_df['success'] == True]\n        \n        # Figure 1: Detection Performance by Attack Type\n        self._plot_detection_performance(successful_df, output_dir)\n        \n        # Figure 2: Latency vs Network Size\n        self._plot_scalability_analysis(successful_df, output_dir)\n        \n        # Figure 3: Overhead Analysis\n        self._plot_overhead_analysis(successful_df, output_dir)\n        \n        # Figure 4: Trust Score Evolution\n        self._plot_trust_analysis(successful_df, output_dir)\n        \n        # Figure 5: Topology Comparison\n        self._plot_topology_comparison(successful_df, output_dir)\n        \n        print(f\"Paper figures generated in: {output_dir}\")\n    \n    def _plot_detection_performance(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Plot detection performance comparison.\"\"\"\n        \n        # Filter out 'none' attack type\n        attack_df = df[df['config'].apply(lambda x: x['attack_type']) != 'none']\n        \n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n        \n        # Precision by attack type\n        attack_types = ['label_flipping', 'model_poisoning', 'byzantine_gradient']\n        precision_data = []\n        for attack_type in attack_types:\n            type_df = attack_df[attack_df['config'].apply(lambda x: x['attack_type']) == attack_type]\n            precision_data.extend(type_df['precision'].values)\n        \n        attack_labels = []\n        for attack_type in attack_types:\n            type_df = attack_df[attack_df['config'].apply(lambda x: x['attack_type']) == attack_type]\n            attack_labels.extend([attack_type.replace('_', '\\n')] * len(type_df))\n        \n        sns.boxplot(x=attack_labels, y=precision_data, ax=ax1)\n        ax1.set_title('Precision by Attack Type')\n        ax1.set_ylabel('Precision')\n        ax1.tick_params(axis='x', rotation=45)\n        \n        # Recall by attack type\n        recall_data = []\n        for attack_type in attack_types:\n            type_df = attack_df[attack_df['config'].apply(lambda x: x['attack_type']) == attack_type]\n            recall_data.extend(type_df['recall'].values)\n        \n        sns.boxplot(x=attack_labels, y=recall_data, ax=ax2)\n        ax2.set_title('Recall by Attack Type')\n        ax2.set_ylabel('Recall')\n        ax2.tick_params(axis='x', rotation=45)\n        \n        # F1-Score by attack type\n        f1_data = []\n        for attack_type in attack_types:\n            type_df = attack_df[attack_df['config'].apply(lambda x: x['attack_type']) == attack_type]\n            f1_data.extend(type_df['f1_score'].values)\n        \n        sns.boxplot(x=attack_labels, y=f1_data, ax=ax3)\n        ax3.set_title('F1-Score by Attack Type')\n        ax3.set_ylabel('F1-Score')\n        ax3.tick_params(axis='x', rotation=45)\n        \n        # Detection latency by attack type\n        latency_data = []\n        latency_labels = []\n        for attack_type in attack_types:\n            type_df = attack_df[attack_df['config'].apply(lambda x: x['attack_type']) == attack_type]\n            latency_subset = type_df[type_df['avg_detection_latency'] != float('inf')]\n            if len(latency_subset) > 0:\n                latency_data.extend(latency_subset['avg_detection_latency'].values)\n                latency_labels.extend([attack_type.replace('_', '\\n')] * len(latency_subset))\n        \n        if latency_data:\n            sns.boxplot(x=latency_labels, y=latency_data, ax=ax4)\n        ax4.set_title('Detection Latency by Attack Type')\n        ax4.set_ylabel('Detection Latency (rounds)')\n        ax4.tick_params(axis='x', rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, \"detection_performance.pdf\"), dpi=300, bbox_inches='tight')\n        plt.savefig(os.path.join(output_dir, \"detection_performance.png\"), dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _plot_scalability_analysis(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Plot scalability analysis.\"\"\"\n        \n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n        \n        # Group by number of actors\n        num_actors_list = sorted(df['config'].apply(lambda x: x['num_actors']).unique())\n        \n        scalability_metrics = {\n            'num_actors': [],\n            'avg_memory': [], 'std_memory': [],\n            'avg_latency': [], 'std_latency': [],\n            'avg_precision': [], 'std_precision': [],\n            'avg_recall': [], 'std_recall': []\n        }\n        \n        for num_actors in num_actors_list:\n            actors_df = df[df['config'].apply(lambda x: x['num_actors']) == num_actors]\n            \n            scalability_metrics['num_actors'].append(num_actors)\n            scalability_metrics['avg_memory'].append(actors_df['avg_memory_mb'].mean())\n            scalability_metrics['std_memory'].append(actors_df['avg_memory_mb'].std())\n            \n            # Only consider experiments with attacks for detection metrics\n            attack_df = actors_df[actors_df['config'].apply(lambda x: x['attack_type']) != 'none']\n            \n            if len(attack_df) > 0:\n                scalability_metrics['avg_precision'].append(attack_df['precision'].mean())\n                scalability_metrics['std_precision'].append(attack_df['precision'].std())\n                scalability_metrics['avg_recall'].append(attack_df['recall'].mean())\n                scalability_metrics['std_recall'].append(attack_df['recall'].std())\n                \n                # Latency for successful detections\n                latency_df = attack_df[attack_df['avg_detection_latency'] != float('inf')]\n                if len(latency_df) > 0:\n                    scalability_metrics['avg_latency'].append(latency_df['avg_detection_latency'].mean())\n                    scalability_metrics['std_latency'].append(latency_df['avg_detection_latency'].std())\n                else:\n                    scalability_metrics['avg_latency'].append(np.nan)\n                    scalability_metrics['std_latency'].append(np.nan)\n            else:\n                scalability_metrics['avg_precision'].append(np.nan)\n                scalability_metrics['std_precision'].append(np.nan)\n                scalability_metrics['avg_recall'].append(np.nan)\n                scalability_metrics['std_recall'].append(np.nan)\n                scalability_metrics['avg_latency'].append(np.nan)\n                scalability_metrics['std_latency'].append(np.nan)\n        \n        # Memory usage vs network size\n        ax1.errorbar(scalability_metrics['num_actors'], scalability_metrics['avg_memory'],\n                    yerr=scalability_metrics['std_memory'], marker='o', capsize=5)\n        ax1.set_xlabel('Number of Actors')\n        ax1.set_ylabel('Average Memory Usage (MB)')\n        ax1.set_title('Memory Scalability')\n        ax1.grid(True, alpha=0.3)\n        \n        # Detection latency vs network size\n        valid_indices = [i for i, val in enumerate(scalability_metrics['avg_latency']) if not np.isnan(val)]\n        if valid_indices:\n            x_vals = [scalability_metrics['num_actors'][i] for i in valid_indices]\n            y_vals = [scalability_metrics['avg_latency'][i] for i in valid_indices]\n            y_errs = [scalability_metrics['std_latency'][i] for i in valid_indices]\n            \n            ax2.errorbar(x_vals, y_vals, yerr=y_errs, marker='s', capsize=5, color='orange')\n        ax2.set_xlabel('Number of Actors')\n        ax2.set_ylabel('Average Detection Latency (rounds)')\n        ax2.set_title('Detection Latency Scalability')\n        ax2.grid(True, alpha=0.3)\n        \n        # Precision vs network size\n        valid_indices = [i for i, val in enumerate(scalability_metrics['avg_precision']) if not np.isnan(val)]\n        if valid_indices:\n            x_vals = [scalability_metrics['num_actors'][i] for i in valid_indices]\n            y_vals = [scalability_metrics['avg_precision'][i] for i in valid_indices]\n            y_errs = [scalability_metrics['std_precision'][i] for i in valid_indices]\n            \n            ax3.errorbar(x_vals, y_vals, yerr=y_errs, marker='^', capsize=5, color='green')\n        ax3.set_xlabel('Number of Actors')\n        ax3.set_ylabel('Average Precision')\n        ax3.set_title('Precision Scalability')\n        ax3.grid(True, alpha=0.3)\n        \n        # Recall vs network size\n        valid_indices = [i for i, val in enumerate(scalability_metrics['avg_recall']) if not np.isnan(val)]\n        if valid_indices:\n            x_vals = [scalability_metrics['num_actors'][i] for i in valid_indices]\n            y_vals = [scalability_metrics['avg_recall'][i] for i in valid_indices]\n            y_errs = [scalability_metrics['std_recall'][i] for i in valid_indices]\n            \n            ax4.errorbar(x_vals, y_vals, yerr=y_errs, marker='d', capsize=5, color='red')\n        ax4.set_xlabel('Number of Actors')\n        ax4.set_ylabel('Average Recall')\n        ax4.set_title('Recall Scalability')\n        ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, \"scalability_analysis.pdf\"), dpi=300, bbox_inches='tight')\n        plt.savefig(os.path.join(output_dir, \"scalability_analysis.png\"), dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _plot_overhead_analysis(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Plot overhead analysis.\"\"\"\n        \n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n        \n        # Memory overhead by dataset\n        datasets = ['mnist', 'cifar10']\n        memory_data = []\n        dataset_labels = []\n        \n        for dataset in datasets:\n            dataset_df = df[df['config'].apply(lambda x: x['dataset']) == dataset]\n            if len(dataset_df) > 0:\n                memory_data.extend(dataset_df['avg_memory_mb'].values)\n                dataset_labels.extend([dataset.upper()] * len(dataset_df))\n        \n        sns.boxplot(x=dataset_labels, y=memory_data, ax=ax1)\n        ax1.set_title('Memory Usage by Dataset')\n        ax1.set_ylabel('Average Memory (MB)')\n        \n        # Trust overhead by topology\n        topologies = ['ring', 'complete', 'line']\n        overhead_data = []\n        topology_labels = []\n        \n        for topology in topologies:\n            topo_df = df[df['config'].apply(lambda x: x['topology']) == topology]\n            if len(topo_df) > 0:\n                overhead_data.extend(topo_df['trust_overhead_mb'].values)\n                topology_labels.extend([topology.title()] * len(topo_df))\n        \n        sns.boxplot(x=topology_labels, y=overhead_data, ax=ax2)\n        ax2.set_title('Trust Overhead by Topology')\n        ax2.set_ylabel('Trust Overhead (MB)')\n        \n        # CPU usage by number of actors\n        num_actors_list = sorted(df['config'].apply(lambda x: x['num_actors']).unique())\n        cpu_means = []\n        cpu_stds = []\n        \n        for num_actors in num_actors_list:\n            actors_df = df[df['config'].apply(lambda x: x['num_actors']) == num_actors]\n            cpu_means.append(actors_df['avg_cpu_percent'].mean())\n            cpu_stds.append(actors_df['avg_cpu_percent'].std())\n        \n        ax3.errorbar(num_actors_list, cpu_means, yerr=cpu_stds, marker='o', capsize=5)\n        ax3.set_xlabel('Number of Actors')\n        ax3.set_ylabel('Average CPU Usage (%)')\n        ax3.set_title('CPU Usage Scalability')\n        ax3.grid(True, alpha=0.3)\n        \n        # Memory vs Trust Overhead scatter\n        ax4.scatter(df['avg_memory_mb'], df['trust_overhead_mb'], alpha=0.6)\n        ax4.set_xlabel('Average Memory Usage (MB)')\n        ax4.set_ylabel('Trust Overhead (MB)')\n        ax4.set_title('Memory vs Trust Overhead')\n        ax4.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, \"overhead_analysis.pdf\"), dpi=300, bbox_inches='tight')\n        plt.savefig(os.path.join(output_dir, \"overhead_analysis.png\"), dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _plot_trust_analysis(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Plot trust score analysis.\"\"\"\n        \n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n        \n        # Trust score separation by attack type\n        attack_df = df[df['config'].apply(lambda x: x['attack_type']) != 'none']\n        attack_types = ['label_flipping', 'model_poisoning', 'byzantine_gradient']\n        \n        separation_data = []\n        attack_labels = []\n        \n        for attack_type in attack_types:\n            type_df = attack_df[attack_df['config'].apply(lambda x: x['attack_type']) == attack_type]\n            if len(type_df) > 0:\n                separation_data.extend(type_df['trust_score_separation'].values)\n                attack_labels.extend([attack_type.replace('_', '\\n')] * len(type_df))\n        \n        sns.boxplot(x=attack_labels, y=separation_data, ax=ax1)\n        ax1.set_title('Trust Score Separation by Attack Type')\n        ax1.set_ylabel('Cohen\\'s d (Trust Separation)')\n        ax1.tick_params(axis='x', rotation=45)\n        \n        # Trust scores: honest vs malicious\n        honest_scores = df['avg_trust_honest'].values\n        malicious_scores = df['avg_trust_malicious'].values\n        \n        ax2.hist(honest_scores, alpha=0.7, label='Honest Nodes', bins=20)\n        ax2.hist(malicious_scores, alpha=0.7, label='Malicious Nodes', bins=20)\n        ax2.set_xlabel('Average Trust Score')\n        ax2.set_ylabel('Frequency')\n        ax2.set_title('Trust Score Distribution')\n        ax2.legend()\n        \n        # Trust separation vs detection performance\n        attack_df_clean = attack_df[attack_df['trust_score_separation'] > 0]\n        if len(attack_df_clean) > 0:\n            ax3.scatter(attack_df_clean['trust_score_separation'], attack_df_clean['f1_score'], alpha=0.6)\n            ax3.set_xlabel('Trust Score Separation (Cohen\\'s d)')\n            ax3.set_ylabel('F1-Score')\n            ax3.set_title('Trust Separation vs Detection Performance')\n            ax3.grid(True, alpha=0.3)\n        \n        # Trust separation by topology\n        topologies = ['ring', 'complete', 'line']\n        separation_by_topo = []\n        topo_labels = []\n        \n        for topology in topologies:\n            topo_df = attack_df[attack_df['config'].apply(lambda x: x['topology']) == topology]\n            if len(topo_df) > 0:\n                separation_by_topo.extend(topo_df['trust_score_separation'].values)\n                topo_labels.extend([topology.title()] * len(topo_df))\n        \n        sns.boxplot(x=topo_labels, y=separation_by_topo, ax=ax4)\n        ax4.set_title('Trust Separation by Topology')\n        ax4.set_ylabel('Cohen\\'s d (Trust Separation)')\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, \"trust_analysis.pdf\"), dpi=300, bbox_inches='tight')\n        plt.savefig(os.path.join(output_dir, \"trust_analysis.png\"), dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def _plot_topology_comparison(self, df: pd.DataFrame, output_dir: str):\n        \"\"\"Plot topology comparison.\"\"\"\n        \n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n        \n        topologies = ['ring', 'complete', 'line']\n        \n        # Detection performance by topology\n        precision_by_topo = []\n        recall_by_topo = []\n        topo_labels_precision = []\n        topo_labels_recall = []\n        \n        for topology in topologies:\n            topo_df = df[df['config'].apply(lambda x: x['topology']) == topology]\n            attack_df = topo_df[topo_df['config'].apply(lambda x: x['attack_type']) != 'none']\n            \n            if len(attack_df) > 0:\n                precision_by_topo.extend(attack_df['precision'].values)\n                topo_labels_precision.extend([topology.title()] * len(attack_df))\n                \n                recall_by_topo.extend(attack_df['recall'].values)\n                topo_labels_recall.extend([topology.title()] * len(attack_df))\n        \n        sns.boxplot(x=topo_labels_precision, y=precision_by_topo, ax=ax1)\n        ax1.set_title('Precision by Topology')\n        ax1.set_ylabel('Precision')\n        \n        sns.boxplot(x=topo_labels_recall, y=recall_by_topo, ax=ax2)\n        ax2.set_title('Recall by Topology')\n        ax2.set_ylabel('Recall')\n        \n        # Resource usage by topology\n        memory_by_topo = []\n        latency_by_topo = []\n        topo_labels_memory = []\n        topo_labels_latency = []\n        \n        for topology in topologies:\n            topo_df = df[df['config'].apply(lambda x: x['topology']) == topology]\n            \n            if len(topo_df) > 0:\n                memory_by_topo.extend(topo_df['avg_memory_mb'].values)\n                topo_labels_memory.extend([topology.title()] * len(topo_df))\n                \n                # Latency for attack experiments only\n                attack_df = topo_df[topo_df['config'].apply(lambda x: x['attack_type']) != 'none']\n                latency_subset = attack_df[attack_df['avg_detection_latency'] != float('inf')]\n                if len(latency_subset) > 0:\n                    latency_by_topo.extend(latency_subset['avg_detection_latency'].values)\n                    topo_labels_latency.extend([topology.title()] * len(latency_subset))\n        \n        sns.boxplot(x=topo_labels_memory, y=memory_by_topo, ax=ax3)\n        ax3.set_title('Memory Usage by Topology')\n        ax3.set_ylabel('Average Memory (MB)')\n        \n        if latency_by_topo:\n            sns.boxplot(x=topo_labels_latency, y=latency_by_topo, ax=ax4)\n        ax4.set_title('Detection Latency by Topology')\n        ax4.set_ylabel('Detection Latency (rounds)')\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, \"topology_comparison.pdf\"), dpi=300, bbox_inches='tight')\n        plt.savefig(os.path.join(output_dir, \"topology_comparison.png\"), dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    def generate_summary_report(self, output_file: str = None):\n        \"\"\"Generate comprehensive summary report.\"\"\"\n        \n        if output_file is None:\n            output_file = os.path.join(self.results_dir, \"experiment_summary_report.md\")\n        \n        successful_df = self.results_df[self.results_df['success'] == True]\n        \n        with open(output_file, 'w') as f:\n            f.write(\"# Trust Monitor Experiment Results Summary\\n\\n\")\n            \n            # Overview\n            f.write(\"## Experiment Overview\\n\\n\")\n            f.write(f\"- **Total Experiments**: {len(self.results_df)}\\n\")\n            f.write(f\"- **Successful Experiments**: {len(successful_df)}\\n\")\n            f.write(f\"- **Success Rate**: {len(successful_df) / len(self.results_df):.1%}\\n\\n\")\n            \n            # Detection Performance Summary\n            attack_df = successful_df[successful_df['config'].apply(lambda x: x['attack_type']) != 'none']\n            \n            f.write(\"## Detection Performance Summary\\n\\n\")\n            f.write(f\"- **Average Precision**: {attack_df['precision'].mean():.3f} ± {attack_df['precision'].std():.3f}\\n\")\n            f.write(f\"- **Average Recall**: {attack_df['recall'].mean():.3f} ± {attack_df['recall'].std():.3f}\\n\")\n            f.write(f\"- **Average F1-Score**: {attack_df['f1_score'].mean():.3f} ± {attack_df['f1_score'].std():.3f}\\n\")\n            \n            latency_df = attack_df[attack_df['avg_detection_latency'] != float('inf')]\n            if len(latency_df) > 0:\n                f.write(f\"- **Average Detection Latency**: {latency_df['avg_detection_latency'].mean():.1f} ± {latency_df['avg_detection_latency'].std():.1f} rounds\\n\")\n            f.write(\"\\n\")\n            \n            # Overhead Summary\n            f.write(\"## Overhead Analysis Summary\\n\\n\")\n            f.write(f\"- **Average Memory Usage**: {successful_df['avg_memory_mb'].mean():.1f} ± {successful_df['avg_memory_mb'].std():.1f} MB\\n\")\n            f.write(f\"- **Average Trust Overhead**: {successful_df['trust_overhead_mb'].mean():.1f} ± {successful_df['trust_overhead_mb'].std():.1f} MB\\n\")\n            f.write(f\"- **Average CPU Usage**: {successful_df['avg_cpu_percent'].mean():.1f} ± {successful_df['avg_cpu_percent'].std():.1f}%\\n\")\n            f.write(f\"- **Communication Overhead**: {successful_df['communication_overhead_percent'].mean():.1f}%\\n\\n\")\n            \n            # Key Findings\n            f.write(\"## Key Findings\\n\\n\")\n            \n            # Best performing attack detection\n            best_precision_attack = attack_df.groupby(attack_df['config'].apply(lambda x: x['attack_type']))['precision'].mean().idxmax()\n            best_recall_attack = attack_df.groupby(attack_df['config'].apply(lambda x: x['attack_type']))['recall'].mean().idxmax()\n            \n            f.write(f\"- **Best Precision**: {best_precision_attack.replace('_', ' ').title()} attacks\\n\")\n            f.write(f\"- **Best Recall**: {best_recall_attack.replace('_', ' ').title()} attacks\\n\")\n            \n            # Most efficient topology\n            topo_memory = successful_df.groupby(successful_df['config'].apply(lambda x: x['topology']))['avg_memory_mb'].mean()\n            most_efficient_topo = topo_memory.idxmin()\n            f.write(f\"- **Most Memory Efficient Topology**: {most_efficient_topo.title()}\\n\")\n            \n            # Trust separation\n            best_separation = attack_df['trust_score_separation'].max()\n            f.write(f\"- **Best Trust Score Separation**: {best_separation:.2f} (Cohen's d)\\n\\n\")\n            \n            f.write(\"## Conclusions\\n\\n\")\n            f.write(\"The trust monitoring system demonstrates:\\n\")\n            f.write(f\"1. High detection accuracy with {attack_df['f1_score'].mean():.1%} average F1-score\\n\")\n            f.write(f\"2. Low overhead with {successful_df['trust_overhead_mb'].mean():.1f}MB average trust monitoring overhead\\n\")\n            f.write(f\"3. Scalable performance across network sizes from {successful_df['config'].apply(lambda x: x['num_actors']).min()} to {successful_df['config'].apply(lambda x: x['num_actors']).max()} nodes\\n\")\n            f.write(f\"4. Effective across different network topologies and attack types\\n\")\n        \n        print(f\"Summary report generated: {output_file}\")\n\n\ndef main():\n    \"\"\"Main function for analysis tool.\"\"\"\n    \n    parser = argparse.ArgumentParser(\n        description=\"Comprehensive Analysis Tool for Trust Monitor Experiments\"\n    )\n    \n    parser.add_argument(\n        \"results_dir\", type=str,\n        help=\"Directory containing experiment results\"\n    )\n    parser.add_argument(\n        \"--tables\", action=\"store_true\",\n        help=\"Generate LaTeX tables for paper\"\n    )\n    parser.add_argument(\n        \"--figures\", action=\"store_true\",\n        help=\"Generate paper figures\"\n    )\n    parser.add_argument(\n        \"--report\", action=\"store_true\",\n        help=\"Generate summary report\"\n    )\n    parser.add_argument(\n        \"--all\", action=\"store_true\",\n        help=\"Generate all outputs (tables, figures, report)\"\n    )\n    \n    args = parser.parse_args()\n    \n    # Create analyzer\n    analyzer = ExperimentAnalyzer(args.results_dir)\n    \n    # Load results\n    analyzer.load_results()\n    \n    # Generate requested outputs\n    if args.all or args.tables:\n        analyzer.generate_paper_tables()\n    \n    if args.all or args.figures:\n        analyzer.generate_paper_figures()\n    \n    if args.all or args.report:\n        analyzer.generate_summary_report()\n    \n    if not any([args.tables, args.figures, args.report, args.all]):\n        print(\"No output requested. Use --tables, --figures, --report, or --all\")\n\n\nif __name__ == \"__main__\":\n    main()